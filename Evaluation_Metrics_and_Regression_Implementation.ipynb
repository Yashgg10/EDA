{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg+gkHgsU86cDh6QM5qGeh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashgg10/EDA/blob/main/Evaluation_Metrics_and_Regression_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression and Machine Learning Key Concepts**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. What does R-squared represent in a regression model?**\n",
        "- R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables.  \n",
        "- Value ranges from 0 to 1:\n",
        "  - **0**: Model explains none of the variance.\n",
        "  - **1**: Model explains all the variance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. What are the assumptions of linear regression?**\n",
        "1. **Linearity**: The relationship between predictors and outcome is linear.\n",
        "2. **Independence**: Residuals are independent of each other.\n",
        "3. **Homoscedasticity**: Residuals have constant variance.\n",
        "4. **Normality**: Residuals are normally distributed.\n",
        "5. **No Multicollinearity**: Predictors are not highly correlated with each other.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. What is the difference between R-squared and Adjusted R-squared?**\n",
        "- **R-squared**: Measures the variance explained by all predictors.\n",
        "- **Adjusted R-squared**: Accounts for the number of predictors, penalizing the addition of irrelevant variables.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Why do we use Mean Squared Error (MSE)?**\n",
        "- MSE measures the average squared difference between predicted and actual values.  \n",
        "- Squaring amplifies larger errors, making it useful for assessing model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. What does an Adjusted R-squared value of 0.85 indicate?**\n",
        "- Approximately 85% of the variance in the dependent variable is explained by the model, adjusted for the number of predictors.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. How do we check for normality of residuals in linear regression?**\n",
        "- **Histogram** of residuals.\n",
        "- **Q-Q Plot**: Compare residuals to a normal distribution.\n",
        "- **Shapiro-Wilk or Kolmogorov-Smirnov Tests**: Statistical tests for normality.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. What is multicollinearity, and how does it impact regression?**\n",
        "- **Multicollinearity** occurs when predictors are highly correlated.  \n",
        "- Impacts:\n",
        "  - Unstable coefficient estimates.\n",
        "  - Inflated standard errors.\n",
        "  - Reduced interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. What is Mean Absolute Error (MAE)?**\n",
        "- MAE is the average absolute difference between predicted and actual values.\n",
        "- Less sensitive to outliers compared to MSE.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. What are the benefits of using an ML pipeline?**\n",
        "- **Automation**: Streamlines data preprocessing and model building.\n",
        "- **Reproducibility**: Ensures consistent results.\n",
        "- **Scalability**: Efficiently handles large datasets.\n",
        "- **Modularity**: Simplifies the addition or removal of steps.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Why is RMSE considered more interpretable than MSE?**\n",
        "- RMSE is in the same units as the target variable, making it easier to understand and compare.\n",
        "\n",
        "---\n",
        "\n",
        "#### **11. What is pickling in Python, and how is it useful in ML?**\n",
        "- **Pickling**: Serializes Python objects into a binary format for storage.  \n",
        "- Usefulness:\n",
        "  - Saves trained models for future use.\n",
        "  - Enables transfer of models across systems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **12. What does a high R-squared value mean?**\n",
        "- Indicates the model explains a large proportion of the variance.\n",
        "- Caution: A high value doesn’t guarantee a good model; check for overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "#### **13. What happens if linear regression assumptions are violated?**\n",
        "- Results may be biased or unreliable:\n",
        "  - Inaccurate coefficients.\n",
        "  - Misleading p-values.\n",
        "  - Poor predictive performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **14. How can we address multicollinearity in regression?**\n",
        "- Use **Variance Inflation Factor (VIF)** to detect it.\n",
        "- Remedies:\n",
        "  - Remove highly correlated predictors.\n",
        "  - Use dimensionality reduction (e.g., PCA).\n",
        "  - Regularization methods like Ridge or Lasso.\n",
        "\n",
        "---\n",
        "\n",
        "#### **15. How can feature selection improve model performance in regression analysis?**\n",
        "- Reduces noise and overfitting.\n",
        "- Improves computational efficiency.\n",
        "- Enhances model interpretability by focusing on the most relevant predictors.\n",
        "\n",
        "---\n",
        "\n",
        "#### **16. How is Adjusted R-squared calculated?**\n",
        "adjusted R2  = [1-(1-rsquare)*n-1/(n-p-1)]\n",
        "#n is no of observation and p is no of predictor variables\n",
        "- **n**: Number of observations.\n",
        "- **k**: Number of predictors.\n",
        "\n",
        "---\n",
        "\n",
        "#### **17. Why is MSE sensitive to outliers?**\n",
        "- Squaring residuals magnifies the impact of larger errors caused by outliers.\n",
        "\n",
        "---\n",
        "\n",
        "#### **18. What is the role of homoscedasticity in linear regression?**\n",
        "- Ensures that residuals have constant variance.\n",
        "- Violations can lead to biased standard errors and unreliable hypothesis tests.\n",
        "\n",
        "---\n",
        "\n",
        "#### **19. What is Root Mean Squared Error (RMSE)?**\n",
        "- RMSE is the square root of the MSE.\n",
        "\\[\n",
        "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
        "\\]\n",
        "- Provides a measure of model accuracy in the same units as the target variable.\n",
        "\n",
        "---\n",
        "\n",
        "#### **20. Why is pickling considered risky?**\n",
        "- **Security risks**: Deserializing untrusted files may execute malicious code.\n",
        "- **Compatibility issues**: Pickle files are Python-specific and version-dependent.\n",
        "\n",
        "---\n",
        "\n",
        "#### **21. What alternatives exist to pickling for saving ML models?**\n",
        "- **Joblib**: Efficient for large numpy arrays.\n",
        "- **ONNX**: Platform-independent model serialization.\n",
        "- **PMML**: Standard format for predictive models.\n",
        "\n",
        "---\n",
        "\n",
        "#### **22. What is heteroscedasticity, and why is it a problem?**\n",
        "- Heteroscedasticity occurs when residual variance is not constant.  \n",
        "- Problems:\n",
        "  - Biased coefficient estimates.\n",
        "  - Invalid hypothesis tests.\n",
        "\n",
        "---\n",
        "\n",
        "#### **23. How can interaction terms enhance a regression model's predictive power?**\n",
        "- Captures relationships where the effect of one predictor depends on another.  \n",
        "- Example: Sales = Price + Advertising + (Price × Advertising)."
      ],
      "metadata": {
        "id": "NdZBUWW5rzEk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5lBIz24rnIt"
      },
      "outputs": [],
      "source": []
    }
  ]
}